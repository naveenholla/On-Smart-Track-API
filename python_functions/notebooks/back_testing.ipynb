{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import math\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize(df):    \n",
    "    if df.empty:\n",
    "        return\n",
    "    if len(df.columns) > 0:\n",
    "        common_names = {\n",
    "            \"Date\": \"date\",\n",
    "            \"Time\": \"time\",\n",
    "            \"Timestamp\": \"timestamp\",\n",
    "            \"Datetime\": \"datetime\",\n",
    "            \"Open\": \"open\",\n",
    "            \"High\": \"high\",\n",
    "            \"Low\": \"low\",\n",
    "            \"Close\": \"close\",\n",
    "            \"Adj Close\": \"adj_close\",\n",
    "            \"Volume\": \"volume\",\n",
    "            \"Dividends\": \"dividends\",\n",
    "            \"Stock Splits\": \"split\",\n",
    "            \"open_price\": \"open\",\n",
    "            \"high_price\": \"high\",\n",
    "            \"low_price\": \"low\",\n",
    "            \"close_price\": \"close\",\n",
    "            \"traded_quantity\": \"volume\",\n",
    "        }\n",
    "        # Preemptively drop the rows that are all NaNs\n",
    "        # Might need to be moved to AnalysisIndicators.__call__() to be\n",
    "        #   toggleable via kwargs.\n",
    "        # df.dropna(axis=0, inplace=True)\n",
    "        # Preemptively rename columns to lowercase\n",
    "        df.rename(columns=common_names, errors=\"ignore\", inplace=True)\n",
    "        \n",
    "        col_types = {\n",
    "            \"open\": float,\n",
    "            \"high\": float,\n",
    "            \"low\": float,\n",
    "            \"close\": float,\n",
    "        }\n",
    "        \n",
    "        df = df.astype(col_types)\n",
    "\n",
    "        # Preemptively lowercase the index\n",
    "        index_name = df.index.name\n",
    "        if index_name is not None:\n",
    "            df.index.rename(index_name.lower(), inplace=True)\n",
    "        else:\n",
    "            df.set_index(pd.DatetimeIndex(df['date']))\n",
    "            \n",
    "        return df\n",
    "    else:\n",
    "        raise AttributeError(f\"[X] No columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "\n",
    "\n",
    "def _create_level_object(row, type):\n",
    "    open_ = row[\"open\"]\n",
    "    high = row[\"high\"]\n",
    "    low = row[\"low\"]\n",
    "    close = row[\"close\"]\n",
    "\n",
    "    levels = []\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_O\"\n",
    "    level[\"level\"] = np.round(open_, 2)\n",
    "    level[\"is_support\"] = True\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_H\"\n",
    "    level[\"level\"] = np.round(high)\n",
    "    level[\"is_support\"] = False\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_L\"\n",
    "    level[\"level\"] = np.round(low)\n",
    "    level[\"is_support\"] = True\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_C\"\n",
    "    level[\"level\"] = np.round(close)\n",
    "    level[\"is_support\"] = False\n",
    "    levels.append(level)\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _current_previous_levels(df, type):\n",
    "    levels = []\n",
    "\n",
    "    level = _create_level_object(df.iloc[-1], f\"C_{type}\")\n",
    "    levels.extend(level)\n",
    "\n",
    "    level = _create_level_object(df.iloc[-2], f\"P_{type}\")\n",
    "    levels.extend(level)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "def monthly_levels(df):\n",
    "    df_values = df.resample(\"M\").agg(\n",
    "        {\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\"}\n",
    "    )\n",
    "    return _current_previous_levels(df_values, \"M\")\n",
    "\n",
    "\n",
    "def weekly_levels(df):\n",
    "    df_values = df.resample(\"W\").agg(\n",
    "        {\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\"}\n",
    "    )\n",
    "    return _current_previous_levels(df_values, \"W\")\n",
    "\n",
    "\n",
    "def daily_levels(df):\n",
    "    return _current_previous_levels(df, \"D\")\n",
    "\n",
    "\n",
    "def firty_two_week_levels(df):\n",
    "    levels = []\n",
    "    df[\"52W H\"] = df[\"high\"].rolling(window=252, center=False).max()\n",
    "    df[\"52W L\"] = df[\"low\"].rolling(window=252, center=False).min()\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"52W_H\"\n",
    "    level[\"level\"] = np.round(df[\"52W H\"].iloc[-1], 2)\n",
    "    level[\"is_support\"] = False\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"52W_L\"\n",
    "    level[\"level\"] = np.round(df[\"52W L\"].iloc[-1], 2)\n",
    "    level[\"is_support\"] = True\n",
    "    levels.append(level)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "def all_time_levels(df):\n",
    "    levels = []\n",
    "    df[\"ATH\"] = df[\"high\"].max()\n",
    "    df[\"ATC\"] = df[\"close\"].max()\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"ATH\"\n",
    "    level[\"level\"] = np.round(df[\"ATH\"].iloc[-1], 2)\n",
    "    level[\"is_support\"] = False\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"ATC\"\n",
    "    level[\"level\"] = np.round(df[\"ATC\"].iloc[-1], 2)\n",
    "    level[\"is_support\"] = False\n",
    "    levels.append(level)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _support(df, index, n1, n2):\n",
    "    # n1 n2 before and after candle index\n",
    "    for i in range(index - n1 + 1, index + 1):\n",
    "        if df[\"low\"][i] > df[\"low\"][i - 1]:\n",
    "            return False\n",
    "\n",
    "    for i in range(index + 1, index + n2 + 1):\n",
    "        if df[\"low\"][i] < df[\"low\"][i - 1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _resistance(df, index, n1, n2):\n",
    "    # n1 n2 before and after candle index\n",
    "    for i in range(index - n1 + 1, index + 1):\n",
    "        if df[\"high\"][i] < df[\"high\"][i - 1]:\n",
    "            return False\n",
    "\n",
    "    for i in range(index + 1, index + n2 + 1):\n",
    "        if df[\"high\"][i] > df[\"high\"][i - 1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# method 1: fractal candlestick pattern\n",
    "# determine bullish fractal\n",
    "def _is_support(df, i):\n",
    "    cond1 = df[\"low\"][i] < df[\"low\"][i - 1]\n",
    "    cond2 = df[\"low\"][i] < df[\"low\"][i + 1]\n",
    "    cond3 = df[\"low\"][i + 1] < df[\"low\"][i + 2]\n",
    "    cond4 = df[\"low\"][i - 1] < df[\"low\"][i - 2]\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "\n",
    "\n",
    "# determine bearish fractal\n",
    "def _is_resistance(df, i):\n",
    "    cond1 = df[\"high\"][i] > df[\"high\"][i - 1]\n",
    "    cond2 = df[\"high\"][i] > df[\"high\"][i + 1]\n",
    "    cond3 = df[\"high\"][i + 1] > df[\"high\"][i + 2]\n",
    "    cond4 = df[\"high\"][i - 1] > df[\"high\"][i - 2]\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "\n",
    "\n",
    "# to make sure the new level area does not exist already\n",
    "def _is_far_from_level(value, levels, df):\n",
    "    # Clean noise in data by discarding a level if it is near another\n",
    "    # (i.e. if distance to the next level is less than the average candle size for any given day - this will give a rough estimate on volatility)\n",
    "    ave = np.mean(df[\"high\"] - df[\"low\"])\n",
    "    return np.sum([abs(value - level) < ave for _, level in levels]) == 0\n",
    "\n",
    "\n",
    "# This function, given a price value, returns True or False depending on if it is too near to some previously discovered key level.\n",
    "def _distance_from_mean(mean, level, unique_levels):\n",
    "    return np.sum([abs(level - y) < mean for y in unique_levels]) == 0\n",
    "\n",
    "\n",
    "def _group_noise(levels, price, mean):\n",
    "    unique_levels = []\n",
    "    previous_number = None\n",
    "\n",
    "    unique_level = {}\n",
    "    unique_level[\"point\"] = 0\n",
    "    unique_level[\"min_point\"] = sys.maxsize\n",
    "    unique_level[\"max_point\"] = 0\n",
    "    unique_level[\"levels\"] = []\n",
    "    unique_level[\"types\"] = []\n",
    "    unique_level[\"dates\"] = []\n",
    "    unique_level[\"is_support\"] = []\n",
    "    for l in levels:\n",
    "        level = l[\"level\"]\n",
    "        type_ = l[\"type\"]\n",
    "        date_ = l[\"date\"] if \"date\" in l else None\n",
    "        is_support = l[\"is_support\"]\n",
    "        if not previous_number or abs(level - previous_number) < mean:\n",
    "            if not previous_number:\n",
    "                previous_number = level\n",
    "\n",
    "            min_ = min(level, unique_level[\"min_point\"])\n",
    "            max_ = max(level, unique_level[\"max_point\"])\n",
    "            unique_level[\"point\"] = max_ if level < price else min_\n",
    "            unique_level[\"min_point\"] = min_\n",
    "            unique_level[\"max_point\"] = max_\n",
    "            unique_level[\"levels\"].append(level)\n",
    "            unique_level[\"types\"].append(type_)\n",
    "            unique_level[\"dates\"].append(date_)\n",
    "            unique_level[\"is_support\"].append(is_support)\n",
    "\n",
    "            continue\n",
    "\n",
    "        unique_levels.append(unique_level)\n",
    "        previous_number = level\n",
    "\n",
    "        unique_level = {}\n",
    "        unique_level[\"point\"] = previous_number\n",
    "        unique_level[\"min_point\"] = level\n",
    "        unique_level[\"max_point\"] = level\n",
    "        unique_level[\"levels\"] = [\n",
    "            level,\n",
    "        ]\n",
    "        unique_level[\"types\"] = [\n",
    "            type_,\n",
    "        ]\n",
    "        unique_level[\"dates\"] = [\n",
    "            date_,\n",
    "        ]\n",
    "        unique_level[\"is_support\"] = [is_support,]\n",
    "\n",
    "    unique_levels.append(unique_level)\n",
    "    return unique_levels\n",
    "\n",
    "\n",
    "def _fractal_candlestick_pattern_sr_2(df, n1=2, n2=2, remove_noise=False):\n",
    "    levels = []\n",
    "    indexes = list(df.index.values)\n",
    "    for i in range(2, df.shape[0] - 2):\n",
    "        index = Timestamp(indexes[i])\n",
    "        if _support(df, i, n1, n2):\n",
    "            l = df[\"low\"][i]\n",
    "            if not remove_noise or _is_far_from_level(l, levels, df):\n",
    "                levels.append((index, l, \"support\"))\n",
    "        elif _resistance(df, i, n1, n2):\n",
    "            l = df[\"high\"][i]\n",
    "            if not remove_noise or _is_far_from_level(l, levels, df):\n",
    "                levels.append((index, l, \"resistance\"))\n",
    "    return levels\n",
    "\n",
    "\n",
    "# method 2: window shifting method\n",
    "def _window_shifting_method_sr(df, window=5, remove_noise=False):\n",
    "    levels = []\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    for i in range(window, len(df) - window):\n",
    "        high_range = df[\"high\"][i - window : i + window - 1]\n",
    "        current_max = high_range.max()\n",
    "        if current_max not in max_list:\n",
    "            max_list = []\n",
    "        max_list.append(current_max)\n",
    "        if len(max_list) == window and (\n",
    "            not remove_noise or _is_far_from_level(current_max, levels, df)\n",
    "        ):\n",
    "            levels.append((high_range.idxmax(), current_max, \"resistance\"))\n",
    "\n",
    "        low_range = df[\"low\"][i - window : i + window]\n",
    "        current_min = low_range.min()\n",
    "        if current_min not in min_list:\n",
    "            min_list = []\n",
    "        min_list.append(current_min)\n",
    "        if len(min_list) == window and (\n",
    "            not remove_noise or _is_far_from_level(current_min, levels, df)\n",
    "        ):\n",
    "            levels.append((low_range.idxmin(), current_min, \"support\"))\n",
    "    return levels\n",
    "\n",
    "\n",
    "def get_support_resistance(df, n1=2, n2=2, window=5):\n",
    "    # n1 n2 before and after candle index\n",
    "    all_pivots_dict = []\n",
    "    levels = _fractal_candlestick_pattern_sr_2(df, n1, n2)\n",
    "    for level in levels:\n",
    "        point = np.round(level[1], 2)\n",
    "\n",
    "        pivot = {}\n",
    "        pivot[\"type\"] = \"SR_FCP\"\n",
    "        pivot[\"date\"] = level[0].to_pydatetime()\n",
    "        pivot[\"level\"] = point\n",
    "        pivot[\"is_support\"] = level[2] == \"support\"\n",
    "        all_pivots_dict.append(pivot)\n",
    "\n",
    "    levels = _window_shifting_method_sr(df, window)\n",
    "    for level in levels:\n",
    "        point = np.round(level[1], 2)\n",
    "\n",
    "        pivot = {}\n",
    "        pivot[\"type\"] = \"SR_WSM\"\n",
    "        pivot[\"date\"] = level[0].to_pydatetime()\n",
    "        pivot[\"level\"] = point\n",
    "        pivot[\"is_support\"] = level[2] == \"support\"\n",
    "        all_pivots_dict.append(pivot)\n",
    "\n",
    "    return all_pivots_dict\n",
    "\n",
    "\n",
    "def _find_nearest_index(levels, value):\n",
    "    array = np.asarray(levels)\n",
    "    idx = (np.abs(levels - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _shrink_list_index(levels, ltp, items_count=10):\n",
    "    idx = _find_nearest_index(levels, ltp)\n",
    "\n",
    "    min_idx = idx - items_count\n",
    "    max_idx = idx + items_count\n",
    "\n",
    "    if min_idx < 0:\n",
    "        min_idx = 0\n",
    "\n",
    "    if max_idx > len(levels):\n",
    "        max_idx = len(levels)\n",
    "\n",
    "    return min_idx, max_idx\n",
    "\n",
    "\n",
    "def get_eod_sr_levels(df_yearly, dfs):\n",
    "    levels = []\n",
    "\n",
    "    ml = all_time_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = firty_two_week_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = monthly_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = weekly_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = daily_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = get_support_resistance(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    for df in dfs:\n",
    "        ml = get_support_resistance(df)\n",
    "        levels.extend(ml)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "def get_intraday_sr_levels(eod_levels, dfs, ltp, offset_mean=None):\n",
    "    levels = eod_levels\n",
    "        \n",
    "    for df in dfs:\n",
    "        ml = get_support_resistance(df)\n",
    "        levels.extend(ml)\n",
    "\n",
    "    sorted_levels = sorted(levels, key=itemgetter(\"level\"), reverse=False)\n",
    "\n",
    "    if not offset_mean:\n",
    "        # Clean noise in data by discarding a level if it is near another\n",
    "        # (i.e. if distance to the next level is less than the average\n",
    "        # candle size for any given day - this will give a rough estimate on volatility)\n",
    "        offset_mean = np.mean(dfs[0][\"high\"] - dfs[0][\"low\"])\n",
    "\n",
    "    unique_levels = _group_noise(sorted_levels, ltp, offset_mean)\n",
    "    points = [x[\"point\"] for x in unique_levels]\n",
    "    min_, max_ = _shrink_list_index(points, ltp)\n",
    "    return unique_levels[min_:max_]\n",
    "\n",
    "def analysis_market_structure(levels):\n",
    "    last_support = sys.maxsize\n",
    "    last_resistance = 0\n",
    "    swing_low = 0\n",
    "    swing_high = 0\n",
    "        \n",
    "    for level in levels:\n",
    "        value = level[\"level\"]\n",
    "        if last_support >= value:\n",
    "            # New Low, so upward swing is no longer valid\n",
    "            swing_low = 0\n",
    "            \n",
    "        if last_resistance <= value:\n",
    "            # New High, so downward swing is no longer valid\n",
    "            swing_high = 0\n",
    "        \n",
    "        \n",
    "        if level[\"is_support\"]:                \n",
    "            if last_support < value and swing_low == 0: \n",
    "                swing_low = last_support\n",
    "            last_support = value\n",
    "        else:\n",
    "            if last_resistance > value and swing_high == 0:\n",
    "                swing_high = last_resistance\n",
    "            last_resistance = value\n",
    "        \n",
    "    return last_support, last_resistance, swing_low, swing_high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(symbol, period=\"2y\", interval=\"1d\", start_date=None, end_date=None):\n",
    "  df = yf.download(tickers=symbol, interval=interval, period=period, start=start_date, end=end_date)\n",
    "  df['Date'] = pd.to_datetime(df.index)\n",
    "  df['Date'] = df['Date'].apply(mpl_dates.date2num)\n",
    "  df = df.loc[:,['Date', 'Open', 'High', 'Low', 'Close']]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "symbol = 'TCS.NS'\n",
    "# symbol = '^NSEBANK'\n",
    "#symbol = \"^NSEI\"\n",
    "#symbol = \"MSFT\"\n",
    "df_y = get_stock_price(symbol, \"max\", \"1d\")\n",
    "df_m = get_stock_price(symbol, \"2y\", \"1mo\")\n",
    "df_w = get_stock_price(symbol, \"2y\", \"1wk\")\n",
    "df_d = get_stock_price(symbol, \"1y\", \"1d\")\n",
    "df_h = get_stock_price(symbol, \"6mo\", \"1h\")\n",
    "df_15m = get_stock_price(symbol, \"1mo\", \"15m\")\n",
    "df_5m = get_stock_price(symbol, \"7d\", \"5m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = sanitize(df_y)\n",
    "df_m = sanitize(df_m)\n",
    "df_w = sanitize(df_w)\n",
    "df_d = sanitize(df_d)\n",
    "df_h = sanitize(df_h)\n",
    "df_15m = sanitize(df_15m)\n",
    "df_5m = sanitize(df_5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = df_15m.iloc[-1][\"close\"]\n",
    "\n",
    "eod_levels = get_eod_sr_levels(df_y, [df_h, df_m, df_w, df_d])\n",
    "r1 = get_intraday_sr_levels(eod_levels, [df_5m, df_15m], price)\n",
    "\n",
    "\n",
    "print(r1)\n",
    "[x[\"point\"] for x in r1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eod_levels = get_eod_sr_levels(df_y, df_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2021, 11, 23, 0, 0),\n",
       "  'level': 3407.8,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2021, 12, 3, 0, 0),\n",
       "  'level': 3665.95,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2021, 12, 6, 0, 0),\n",
       "  'level': 3522.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2021, 12, 30, 0, 0),\n",
       "  'level': 3680.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 1, 4, 0, 0),\n",
       "  'level': 3889.15,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 1, 6, 0, 0),\n",
       "  'level': 3772.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 1, 10, 0, 0),\n",
       "  'level': 3978.0,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 1, 17, 0, 0),\n",
       "  'level': 4043.0,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 1, 27, 0, 0),\n",
       "  'level': 3625.1,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 2, 3, 0, 0),\n",
       "  'level': 3882.5,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 2, 8, 0, 0),\n",
       "  'level': 3721.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 2, 24, 0, 0),\n",
       "  'level': 3391.1,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 3, 3, 0, 0),\n",
       "  'level': 3578.6,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 3, 10, 0, 0),\n",
       "  'level': 3684.0,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 3, 14, 0, 0),\n",
       "  'level': 3580.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 3, 25, 0, 0),\n",
       "  'level': 3779.5,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 3, 28, 0, 0),\n",
       "  'level': 3661.2,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 4, 19, 0, 0),\n",
       "  'level': 3439.15,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 4, 21, 0, 0),\n",
       "  'level': 3644.0,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 5, 9, 0, 0),\n",
       "  'level': 3346.85,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 5, 12, 0, 0),\n",
       "  'level': 3449.7,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 5, 23, 0, 0),\n",
       "  'level': 3338.9,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 5, 25, 0, 0),\n",
       "  'level': 3154.05,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 6, 3, 0, 0),\n",
       "  'level': 3477.3,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 6, 17, 0, 0),\n",
       "  'level': 3023.85,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 6, 27, 0, 0),\n",
       "  'level': 3362.9,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 7, 15, 0, 0),\n",
       "  'level': 2953.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 7, 22, 0, 0),\n",
       "  'level': 3197.0,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 7, 26, 0, 0),\n",
       "  'level': 3096.5,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 8, 1, 0, 0),\n",
       "  'level': 3271.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 8, 11, 0, 0),\n",
       "  'level': 3428.7,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_WSM',\n",
       "  'date': datetime.datetime(2022, 8, 29, 0, 0),\n",
       "  'level': 3081.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 8, 30, 0, 0),\n",
       "  'level': 3226.5,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 9, 20, 0, 0),\n",
       "  'level': 3079.95,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 9, 26, 0, 0),\n",
       "  'level': 2926.1,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 10, 10, 0, 0),\n",
       "  'level': 3005.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 10, 20, 0, 0),\n",
       "  'level': 3105.0,\n",
       "  'is_support': True},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 11, 2, 0, 0),\n",
       "  'level': 3270.0,\n",
       "  'is_support': False},\n",
       " {'type': 'SR_FCP',\n",
       "  'date': datetime.datetime(2022, 11, 10, 0, 0),\n",
       "  'level': 3170.0,\n",
       "  'is_support': True}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_d\n",
    "levels = get_support_resistance(df, n1=1, n2=2)\n",
    "sorted_levels = sorted(levels, key=itemgetter(\"date\"), reverse=False)\n",
    "filtered_levels = []\n",
    "previous_support_level = 0\n",
    "previous_resistance_level = 0\n",
    "\n",
    "mean = np.mean(df['high'] - df['low'])\n",
    "for i in sorted_levels:\n",
    "  level = i[\"level\"]\n",
    "  is_support = i[\"is_support\"]\n",
    "  if is_support and abs(level - previous_support_level) > mean:\n",
    "    filtered_levels.append(i)\n",
    "    previous_support_level = level\n",
    "  elif not is_support and abs(level - previous_resistance_level) > mean:\n",
    "    filtered_levels.append(i)\n",
    "    previous_resistance_level = level\n",
    "    \n",
    "filtered_levels\n",
    "\n",
    "# r = analysis_market_structure(filtered_levels)\n",
    "# r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**_some_ markdown** and an [internal reference](render/output/markdown)!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown('**_some_ markdown** and an [internal reference](render/output/markdown)!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e136d_row0_col0, #T_e136d_row0_col1, #T_e136d_row0_col2, #T_e136d_row1_col0, #T_e136d_row1_col3, #T_e136d_row1_col4, #T_e136d_row2_col0, #T_e136d_row2_col2, #T_e136d_row2_col3, #T_e136d_row3_col0, #T_e136d_row3_col1, #T_e136d_row3_col2, #T_e136d_row3_col3, #T_e136d_row3_col4, #T_e136d_row4_col0, #T_e136d_row4_col1, #T_e136d_row4_col2, #T_e136d_row4_col3, #T_e136d_row4_col4, #T_e136d_row5_col0, #T_e136d_row5_col2, #T_e136d_row5_col3, #T_e136d_row6_col0, #T_e136d_row6_col1, #T_e136d_row6_col4, #T_e136d_row7_col0, #T_e136d_row7_col1, #T_e136d_row7_col2, #T_e136d_row7_col4, #T_e136d_row8_col0, #T_e136d_row9_col2, #T_e136d_row9_col4 {\n",
       "  color: white;\n",
       "}\n",
       "#T_e136d_row0_col3, #T_e136d_row0_col4, #T_e136d_row1_col1, #T_e136d_row1_col2, #T_e136d_row2_col1, #T_e136d_row5_col1, #T_e136d_row5_col4, #T_e136d_row6_col3, #T_e136d_row7_col3, #T_e136d_row8_col2, #T_e136d_row8_col4, #T_e136d_row9_col1, #T_e136d_row9_col3 {\n",
       "  color: red;\n",
       "}\n",
       "#T_e136d_row2_col4, #T_e136d_row6_col2, #T_e136d_row8_col1, #T_e136d_row8_col3, #T_e136d_row9_col0 {\n",
       "  color: white;\n",
       "  background-color: grey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e136d\" style=\"font-size: 10px\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e136d_level0_col0\" class=\"col_heading level0 col0\" >A</th>\n",
       "      <th id=\"T_e136d_level0_col1\" class=\"col_heading level0 col1\" >B</th>\n",
       "      <th id=\"T_e136d_level0_col2\" class=\"col_heading level0 col2\" >C</th>\n",
       "      <th id=\"T_e136d_level0_col3\" class=\"col_heading level0 col3\" >D</th>\n",
       "      <th id=\"T_e136d_level0_col4\" class=\"col_heading level0 col4\" >E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e136d_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_e136d_row0_col1\" class=\"data row0 col1\" >1.329212</td>\n",
       "      <td id=\"T_e136d_row0_col2\" class=\"data row0 col2\" >nan</td>\n",
       "      <td id=\"T_e136d_row0_col3\" class=\"data row0 col3\" >-0.316280</td>\n",
       "      <td id=\"T_e136d_row0_col4\" class=\"data row0 col4\" >-0.990810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e136d_row1_col0\" class=\"data row1 col0\" >2.000000</td>\n",
       "      <td id=\"T_e136d_row1_col1\" class=\"data row1 col1\" >-1.070816</td>\n",
       "      <td id=\"T_e136d_row1_col2\" class=\"data row1 col2\" >-1.438713</td>\n",
       "      <td id=\"T_e136d_row1_col3\" class=\"data row1 col3\" >0.564417</td>\n",
       "      <td id=\"T_e136d_row1_col4\" class=\"data row1 col4\" >0.295722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e136d_row2_col0\" class=\"data row2 col0\" >3.000000</td>\n",
       "      <td id=\"T_e136d_row2_col1\" class=\"data row2 col1\" >-1.626404</td>\n",
       "      <td id=\"T_e136d_row2_col2\" class=\"data row2 col2\" >0.219565</td>\n",
       "      <td id=\"T_e136d_row2_col3\" class=\"data row2 col3\" >0.678805</td>\n",
       "      <td id=\"T_e136d_row2_col4\" class=\"data row2 col4\" >1.889273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e136d_row3_col0\" class=\"data row3 col0\" >4.000000</td>\n",
       "      <td id=\"T_e136d_row3_col1\" class=\"data row3 col1\" >0.961538</td>\n",
       "      <td id=\"T_e136d_row3_col2\" class=\"data row3 col2\" >0.104011</td>\n",
       "      <td id=\"T_e136d_row3_col3\" class=\"data row3 col3\" >nan</td>\n",
       "      <td id=\"T_e136d_row3_col4\" class=\"data row3 col4\" >0.850229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e136d_row4_col0\" class=\"data row4 col0\" >5.000000</td>\n",
       "      <td id=\"T_e136d_row4_col1\" class=\"data row4 col1\" >1.453425</td>\n",
       "      <td id=\"T_e136d_row4_col2\" class=\"data row4 col2\" >1.057737</td>\n",
       "      <td id=\"T_e136d_row4_col3\" class=\"data row4 col3\" >0.165562</td>\n",
       "      <td id=\"T_e136d_row4_col4\" class=\"data row4 col4\" >0.515018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e136d_row5_col0\" class=\"data row5 col0\" >6.000000</td>\n",
       "      <td id=\"T_e136d_row5_col1\" class=\"data row5 col1\" >-1.336936</td>\n",
       "      <td id=\"T_e136d_row5_col2\" class=\"data row5 col2\" >0.562861</td>\n",
       "      <td id=\"T_e136d_row5_col3\" class=\"data row5 col3\" >1.392855</td>\n",
       "      <td id=\"T_e136d_row5_col4\" class=\"data row5 col4\" >-0.063328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e136d_row6_col0\" class=\"data row6 col0\" >7.000000</td>\n",
       "      <td id=\"T_e136d_row6_col1\" class=\"data row6 col1\" >0.121668</td>\n",
       "      <td id=\"T_e136d_row6_col2\" class=\"data row6 col2\" >1.207603</td>\n",
       "      <td id=\"T_e136d_row6_col3\" class=\"data row6 col3\" >-0.002040</td>\n",
       "      <td id=\"T_e136d_row6_col4\" class=\"data row6 col4\" >1.627796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e136d_row7_col0\" class=\"data row7 col0\" >8.000000</td>\n",
       "      <td id=\"T_e136d_row7_col1\" class=\"data row7 col1\" >0.354493</td>\n",
       "      <td id=\"T_e136d_row7_col2\" class=\"data row7 col2\" >1.037528</td>\n",
       "      <td id=\"T_e136d_row7_col3\" class=\"data row7 col3\" >-0.385684</td>\n",
       "      <td id=\"T_e136d_row7_col4\" class=\"data row7 col4\" >0.519818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e136d_row8_col0\" class=\"data row8 col0\" >9.000000</td>\n",
       "      <td id=\"T_e136d_row8_col1\" class=\"data row8 col1\" >1.686583</td>\n",
       "      <td id=\"T_e136d_row8_col2\" class=\"data row8 col2\" >-1.325963</td>\n",
       "      <td id=\"T_e136d_row8_col3\" class=\"data row8 col3\" >1.428984</td>\n",
       "      <td id=\"T_e136d_row8_col4\" class=\"data row8 col4\" >-2.089354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e136d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e136d_row9_col0\" class=\"data row9 col0\" >10.000000</td>\n",
       "      <td id=\"T_e136d_row9_col1\" class=\"data row9 col1\" >-0.129820</td>\n",
       "      <td id=\"T_e136d_row9_col2\" class=\"data row9 col2\" >0.631523</td>\n",
       "      <td id=\"T_e136d_row9_col3\" class=\"data row9 col3\" >-0.586538</td>\n",
       "      <td id=\"T_e136d_row9_col4\" class=\"data row9 col4\" >0.290720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25af10731f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(24)\n",
    "df = pd.DataFrame({'A': np.linspace(1, 10, 10)})\n",
    "df = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\n",
    "               axis=1)\n",
    "df.iloc[3, 3] = np.nan\n",
    "df.iloc[0, 2] = np.nan\n",
    "\n",
    "def color_negative_red(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: red'` for negative\n",
    "    strings, black otherwise.\n",
    "    \"\"\"\n",
    "    color = 'red' if val < 0 else 'white'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: grey' if v else '' for v in is_max]\n",
    "\n",
    "df.style.\\\n",
    "    applymap(color_negative_red).\\\n",
    "    apply(highlight_max).\\\n",
    "    set_table_attributes('style=\"font-size: 10px\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52f058016e79b0cb6f4bc1fb0cbaec4eb0fb60249ee39701686a66fe42631fb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
