{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import math\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "import matplotlib.dates as mpl_dates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sanitize(df):    \n",
    "    if df.empty:\n",
    "        return\n",
    "    if len(df.columns) > 0:\n",
    "        common_names = {\n",
    "            \"Date\": \"date\",\n",
    "            \"Time\": \"time\",\n",
    "            \"Timestamp\": \"timestamp\",\n",
    "            \"Datetime\": \"datetime\",\n",
    "            \"Open\": \"open\",\n",
    "            \"High\": \"high\",\n",
    "            \"Low\": \"low\",\n",
    "            \"Close\": \"close\",\n",
    "            \"Adj Close\": \"adj_close\",\n",
    "            \"Volume\": \"volume\",\n",
    "            \"Dividends\": \"dividends\",\n",
    "            \"Stock Splits\": \"split\",\n",
    "            \"open_price\": \"open\",\n",
    "            \"high_price\": \"high\",\n",
    "            \"low_price\": \"low\",\n",
    "            \"close_price\": \"close\",\n",
    "            \"traded_quantity\": \"volume\",\n",
    "        }\n",
    "        # Preemptively drop the rows that are all NaNs\n",
    "        # Might need to be moved to AnalysisIndicators.__call__() to be\n",
    "        #   toggleable via kwargs.\n",
    "        # df.dropna(axis=0, inplace=True)\n",
    "        # Preemptively rename columns to lowercase\n",
    "        df.rename(columns=common_names, errors=\"ignore\", inplace=True)\n",
    "        \n",
    "        col_types = {\n",
    "            \"open\": float,\n",
    "            \"high\": float,\n",
    "            \"low\": float,\n",
    "            \"close\": float,\n",
    "        }\n",
    "        \n",
    "        df = df.astype(col_types)\n",
    "\n",
    "        # Preemptively lowercase the index\n",
    "        index_name = df.index.name\n",
    "        if index_name is not None:\n",
    "            df.index.rename(index_name.lower(), inplace=True)\n",
    "        else:\n",
    "            df.set_index(pd.DatetimeIndex(df['date']))\n",
    "            \n",
    "        return df\n",
    "    else:\n",
    "        raise AttributeError(f\"[X] No columns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "\n",
    "\n",
    "def _create_level_object(row, type):\n",
    "    open_ = row[\"open\"]\n",
    "    high = row[\"high\"]\n",
    "    low = row[\"low\"]\n",
    "    close = row[\"close\"]\n",
    "\n",
    "    levels = []\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_O\"\n",
    "    level[\"level\"] = np.round(open_, 2)\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_H\"\n",
    "    level[\"level\"] = np.round(high)\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_L\"\n",
    "    level[\"level\"] = np.round(low)\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"{type}_C\"\n",
    "    level[\"level\"] = np.round(close)\n",
    "    levels.append(level)\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _current_previous_levels(df, type):\n",
    "    levels = []\n",
    "\n",
    "    level = _create_level_object(df.iloc[-1], f\"C_{type}\")\n",
    "    levels.extend(level)\n",
    "\n",
    "    level = _create_level_object(df.iloc[-2], f\"P_{type}\")\n",
    "    levels.extend(level)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "def monthly_levels(df):\n",
    "    df_values = df.resample(\"M\").agg(\n",
    "        {\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\"}\n",
    "    )\n",
    "    return _current_previous_levels(df_values, \"M\")\n",
    "\n",
    "\n",
    "def weekly_levels(df):\n",
    "    df_values = df.resample(\"W\").agg(\n",
    "        {\"open\": \"first\", \"high\": \"max\", \"low\": \"min\", \"close\": \"last\"}\n",
    "    )\n",
    "    return _current_previous_levels(df_values, \"W\")\n",
    "\n",
    "def daily_levels(df):\n",
    "    return _current_previous_levels(df, \"D\")\n",
    "\n",
    "\n",
    "def firty_two_week_levels(df):\n",
    "    levels = []\n",
    "    df[\"52W H\"] = df[\"high\"].rolling(window=252, center=False).max()\n",
    "    df[\"52W L\"] = df[\"low\"].rolling(window=252, center=False).min()\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"52W_H\"\n",
    "    level[\"level\"] = np.round(df[\"52W H\"].iloc[-1], 2)\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"52W_L\"\n",
    "    level[\"level\"] = np.round(df[\"52W L\"].iloc[-1], 2)\n",
    "    levels.append(level)\n",
    "\n",
    "    return levels\n",
    "\n",
    "def all_time_levels(df):\n",
    "    levels = []\n",
    "    df[\"ATH\"] = df[\"high\"].max()\n",
    "    df[\"ATC\"] = df[\"close\"].max()\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"ATH\"\n",
    "    level[\"level\"] = np.round(df[\"ATH\"].iloc[-1], 2)\n",
    "    levels.append(level)\n",
    "\n",
    "    level = {}\n",
    "    level[\"type\"] = f\"ATC\"\n",
    "    level[\"level\"] = np.round(df[\"ATC\"].iloc[-1], 2)\n",
    "    levels.append(level)\n",
    "\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _support(df, index, n1, n2):\n",
    "    # n1 n2 before and after candle index\n",
    "    for i in range(index - n1 + 1, index + 1):\n",
    "        if df[\"low\"][i] > df[\"low\"][i - 1]:\n",
    "            return False\n",
    "\n",
    "    for i in range(index + 1, index + n2 + 1):\n",
    "        if df[\"low\"][i] < df[\"low\"][i - 1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _resistance(df, index, n1, n2):\n",
    "    # n1 n2 before and after candle index\n",
    "    for i in range(index - n1 + 1, index + 1):\n",
    "        if df[\"high\"][i] < df[\"high\"][i - 1]:\n",
    "            return False\n",
    "\n",
    "    for i in range(index + 1, index + n2 + 1):\n",
    "        if df[\"high\"][i] > df[\"high\"][i - 1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# method 1: fractal candlestick pattern\n",
    "# determine bullish fractal\n",
    "def _is_support(df, i):\n",
    "    cond1 = df[\"low\"][i] < df[\"low\"][i - 1]\n",
    "    cond2 = df[\"low\"][i] < df[\"low\"][i + 1]\n",
    "    cond3 = df[\"low\"][i + 1] < df[\"low\"][i + 2]\n",
    "    cond4 = df[\"low\"][i - 1] < df[\"low\"][i - 2]\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "\n",
    "\n",
    "# determine bearish fractal\n",
    "def _is_resistance(df, i):\n",
    "    cond1 = df[\"high\"][i] > df[\"high\"][i - 1]\n",
    "    cond2 = df[\"high\"][i] > df[\"high\"][i + 1]\n",
    "    cond3 = df[\"high\"][i + 1] > df[\"high\"][i + 2]\n",
    "    cond4 = df[\"high\"][i - 1] > df[\"high\"][i - 2]\n",
    "    return cond1 and cond2 and cond3 and cond4\n",
    "\n",
    "\n",
    "# to make sure the new level area does not exist already\n",
    "def _is_far_from_level(value, levels, df):\n",
    "    # Clean noise in data by discarding a level if it is near another\n",
    "    # (i.e. if distance to the next level is less than the average candle size for any given day - this will give a rough estimate on volatility)\n",
    "    ave = np.mean(df[\"high\"] - df[\"low\"])\n",
    "    return np.sum([abs(value - level) < ave for _, level in levels]) == 0\n",
    "\n",
    "\n",
    "# This function, given a price value, returns True or False depending on if it is too near to some previously discovered key level.\n",
    "def _distance_from_mean(mean, level, unique_levels):\n",
    "    return np.sum([abs(level - y) < mean for y in unique_levels]) == 0\n",
    "\n",
    "\n",
    "def _mark_noise(df, levels, price):\n",
    "    # Clean noise in data by discarding a level if it is near another\n",
    "    # (i.e. if distance to the next level is less than the average candle size for any given day - this will give a rough estimate on volatility)\n",
    "    mean = np.mean(df[\"high\"] - df[\"low\"])\n",
    "\n",
    "    unique_levels = []\n",
    "    previous_number = None\n",
    "    \n",
    "    unique_level = {}    \n",
    "    unique_level[\"point\"] = 0\n",
    "    unique_level[\"min_point\"] = 0\n",
    "    unique_level[\"max_point\"] = 0\n",
    "    unique_level[\"levels\"] = []\n",
    "    unique_level[\"types\"] = []\n",
    "    unique_level[\"dates\"] = []\n",
    "    for l in levels: \n",
    "        level = l[\"level\"]\n",
    "        type_ = l[\"type\"]\n",
    "        date_ = l[\"date\"] if \"date\" in l else None\n",
    "        if not previous_number or abs(level - previous_number) < mean:\n",
    "            previous_number = level\n",
    "            \n",
    "            min_ = min(level, unique_level[\"min_point\"])\n",
    "            max_ = max(level, unique_level[\"max_point\"])\n",
    "            unique_level[\"point\"] = max_ if level < price else min_\n",
    "            unique_level[\"min_point\"] = min_\n",
    "            unique_level[\"max_point\"] = max_\n",
    "            unique_level[\"levels\"].append(level)\n",
    "            unique_level[\"types\"].append(type_)\n",
    "            unique_level[\"dates\"].append(date_)\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        unique_levels.append(unique_level)\n",
    "        previous_number = level\n",
    "        \n",
    "        unique_level = {}\n",
    "        unique_level[\"point\"] = previous_number            \n",
    "        unique_level[\"min_point\"] = level\n",
    "        unique_level[\"max_point\"] = level\n",
    "        unique_level[\"levels\"] = [level,]\n",
    "        unique_level[\"types\"] = [type_,]\n",
    "        unique_level[\"dates\"] = [date_,]\n",
    "        \n",
    "    unique_levels.append(unique_level)\n",
    "    return unique_levels\n",
    "\n",
    "# method 1: fractal candlestick pattern\n",
    "def _fractal_candlestick_pattern_sr(df, remove_noise=False):\n",
    "    levels = []\n",
    "    indexes = list(df.index.values)\n",
    "    for i in range(2, df.shape[0] - 2):\n",
    "        index = Timestamp(indexes[i])\n",
    "        if _is_support(df, i):\n",
    "            l = df[\"low\"][i]\n",
    "            if not remove_noise or _is_far_from_level(l, levels, df):\n",
    "                levels.append((index, l))\n",
    "        elif _is_resistance(df, i):\n",
    "            l = df[\"high\"][i]\n",
    "            if not remove_noise or _is_far_from_level(l, levels, df):\n",
    "                levels.append((index, l))\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _fractal_candlestick_pattern_sr_2(df, n1=2, n2=2, remove_noise=False):\n",
    "    levels = []\n",
    "    indexes = list(df.index.values)\n",
    "    for i in range(2, df.shape[0] - 2):\n",
    "        index = Timestamp(indexes[i])\n",
    "        if _support(df, i, n1, n2):\n",
    "            l = df[\"low\"][i]\n",
    "            if not remove_noise or _is_far_from_level(l, levels, df):\n",
    "                levels.append((index, l))\n",
    "        elif _resistance(df, i, n1, n2):\n",
    "            l = df[\"high\"][i]\n",
    "            if not remove_noise or _is_far_from_level(l, levels, df):\n",
    "                levels.append((index, l))\n",
    "    return levels\n",
    "\n",
    "\n",
    "# method 2: window shifting method\n",
    "def _window_shifting_method_sr(df, window=5, remove_noise=False):\n",
    "    levels = []\n",
    "    max_list = []\n",
    "    min_list = []\n",
    "    for i in range(window, len(df) - window):\n",
    "        high_range = df[\"high\"][i - window : i + window - 1]\n",
    "        current_max = high_range.max()\n",
    "        if current_max not in max_list:\n",
    "            max_list = []\n",
    "        max_list.append(current_max)\n",
    "        if len(max_list) == window and (not remove_noise or _is_far_from_level(current_max, levels, df)):\n",
    "            levels.append((high_range.idxmax(), current_max))\n",
    "\n",
    "        low_range = df[\"low\"][i - window : i + window]\n",
    "        current_min = low_range.min()\n",
    "        if current_min not in min_list:\n",
    "            min_list = []\n",
    "        min_list.append(current_min)\n",
    "        if len(min_list) == window and (not remove_noise or _is_far_from_level(current_min, levels, df)):\n",
    "            levels.append((low_range.idxmin(), current_min))\n",
    "    return levels\n",
    "\n",
    "\n",
    "def _get_support_resistance(df, n1=2, n2=2, window=5):\n",
    "    # n1 n2 before and after candle index\n",
    "    all_pivots_dict = []\n",
    "    levels = _fractal_candlestick_pattern_sr_2(df, n1, n2)\n",
    "    for level in levels:\n",
    "        point = np.round(level[1], 2)\n",
    "\n",
    "        pivot = {}\n",
    "        pivot[\"type\"] = \"SR_FCP\"\n",
    "        pivot[\"date\"] = level[0].to_pydatetime()\n",
    "        pivot[\"level\"] = point\n",
    "        all_pivots_dict.append(pivot)\n",
    "\n",
    "    levels = _window_shifting_method_sr(df, window)\n",
    "    for level in levels:\n",
    "        point = np.round(level[1], 2)\n",
    "\n",
    "        pivot = {}\n",
    "        pivot[\"type\"] = \"SR_WSM\"\n",
    "        pivot[\"date\"] = level[0].to_pydatetime()\n",
    "        pivot[\"level\"] = point\n",
    "        all_pivots_dict.append(pivot)\n",
    "\n",
    "    return all_pivots_dict\n",
    "\n",
    "\n",
    "def _find_nearest_index(levels, value):\n",
    "    array = np.asarray(levels)\n",
    "    idx = (np.abs(levels - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _shrink_list_index(levels, ltp, items_count=10):\n",
    "    idx = _find_nearest_index(levels, ltp)\n",
    "\n",
    "    min_idx = idx - items_count\n",
    "    max_idx = idx + items_count\n",
    "\n",
    "    if min_idx < 0:\n",
    "        min_idx = 0\n",
    "\n",
    "    if max_idx > len(levels):\n",
    "        max_idx = len(levels)\n",
    "\n",
    "    return min_idx, max_idx\n",
    "\n",
    "\n",
    "def get_all_support_and_resistance(df_yearly, df_hourly, df_15m, df_5m):\n",
    "    levels = []\n",
    "    \n",
    "    ml = all_time_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = firty_two_week_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = monthly_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = weekly_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "    \n",
    "    ml = daily_levels(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    ml = _get_support_resistance(df_yearly)\n",
    "    levels.extend(ml)\n",
    "\n",
    "    if not df_hourly.empty:\n",
    "        ml = _get_support_resistance(df_hourly)\n",
    "        levels.extend(ml)\n",
    "\n",
    "    if not df_15m.empty:\n",
    "        ml = _get_support_resistance(df_15m)\n",
    "        levels.extend(ml)\n",
    "\n",
    "    sorted_levels = sorted(levels, key=itemgetter(\"level\"), reverse=False)\n",
    "\n",
    "    df_mean = df_yearly\n",
    "    if not df_hourly.empty:\n",
    "        df_mean = df_hourly\n",
    "\n",
    "    if not df_15m.empty:\n",
    "        df_mean = df_15m\n",
    "\n",
    "    if not df_5m.empty:\n",
    "        df_mean = df_5m\n",
    "\n",
    "    price = df_mean.iloc[-1][\"close\"]\n",
    "\n",
    "    unique_levels = _mark_noise(df_mean, sorted_levels, price)\n",
    "    \n",
    "    points = [x[\"point\"] for x in unique_levels]\n",
    "    \n",
    "    min_, max_ = _shrink_list_index(points, price)\n",
    "    return unique_levels[min_:max_]\n",
    "    #return _shrink_list(unique_levels, price), sorted_levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(symbol, period=\"2y\", interval=\"1d\", start_date=None, end_date=None):\n",
    "  df = yf.download(tickers=symbol, interval=interval, period=period, start=start_date, end=end_date)\n",
    "  df['Date'] = pd.to_datetime(df.index)\n",
    "  df['Date'] = df['Date'].apply(mpl_dates.date2num)\n",
    "  df = df.loc[:,['Date', 'Open', 'High', 'Low', 'Close']]\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# symbol = 'TCS.NS'\n",
    "# symbol = '^NSEBANK'\n",
    "symbol = \"^NSEI\"\n",
    "df_y = get_stock_price(symbol, \"max\", \"1d\")\n",
    "df_h = get_stock_price(symbol, \"6mo\", \"1h\")\n",
    "df_15m = get_stock_price(symbol, \"1mo\", \"15m\")\n",
    "df_5m = get_stock_price(symbol, \"7d\", \"5m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = sanitize(df_y)\n",
    "df_h = sanitize(df_h)\n",
    "df_15m = sanitize(df_15m)\n",
    "df_5m = sanitize(df_5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17973.85,\n",
       " 18064.9,\n",
       " 18116.0,\n",
       " 18133.8,\n",
       " 18177.9,\n",
       " 18212.95,\n",
       " 18254.35,\n",
       " 18288.25,\n",
       " 18311.8,\n",
       " 18376.6,\n",
       " 18403.0,\n",
       " 18427.95,\n",
       " 18477.05,\n",
       " 18604.45]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = get_all_support_and_resistance(df_y, df_h, df_15m, df_5m)\n",
    "\n",
    "[x[\"point\"] for x in r1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52f058016e79b0cb6f4bc1fb0cbaec4eb0fb60249ee39701686a66fe42631fb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
